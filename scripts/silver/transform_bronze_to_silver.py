import yaml
import os
import pandas as pd
from google.cloud import storage, bigquery
from google.oauth2 import service_account
from pandas_gbq import to_gbq
from pyspark.sql import SparkSession
from pyspark.sql.functions import col


# üîπ Fun√ß√£o para carregar configura√ß√µes YAML
def load_yaml(yaml_path):
    with open(yaml_path, 'r') as file:
        config = yaml.safe_load(file)

    # Criar credenciais diretamente a partir do YAML
    credentials = service_account.Credentials.from_service_account_file(config["CREDENTIALS_PATH"])

    return config, credentials


# üîπ Fun√ß√£o para encontrar a √∫ltima parti√ß√£o no GCS
def get_latest_partition(storage_client, bucket_name, table_name):
    bucket = storage_client.bucket(bucket_name)
    blobs = bucket.list_blobs(prefix=f"bronze/{table_name}/")

    partitions = sorted(set(blob.name.split('/')[2] for blob in blobs if "dt_extract=" in blob.name))

    if not partitions:
        raise FileNotFoundError("‚ùå Nenhuma parti√ß√£o v√°lida encontrada no GCS!")

    latest_partition = partitions[-1]
    print(f"üìå √öltima parti√ß√£o encontrada: {latest_partition}")
    return latest_partition


# üîπ Fun√ß√£o para carregar JSON do GCS diretamente no PySpark
def load_json_from_gcs(storage_client, spark, bucket_name, table_name, latest_partition):
    gcs_path = f"bronze/{table_name}/{latest_partition}/{table_name}.json"
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(gcs_path)
    json_data = blob.download_as_text()

    # Criar um RDD e converter para DataFrame PySpark
    rdd = spark.sparkContext.parallelize([json_data])
    df_spark = spark.read.json(rdd)

    # Exibir o schema e amostra de dados
    df_spark.printSchema()
    return df_spark


# üîπ Fun√ß√£o para aplicar transforma√ß√µes no DataFrame PySpark
def apply_transforms(df_spark, transformations):
    for column in transformations['columns']:
        column_name = column['name']
        new_name = column.get('rename_to', column_name)
        data_type = column['type']

        if data_type == "string":
            df_spark = df_spark.withColumn(new_name, col(column_name).cast("string"))
        elif data_type == "float":
            df_spark = df_spark.withColumn(new_name, col(column_name).cast("float"))
        elif data_type == "date":
            df_spark = df_spark.withColumn(new_name, col(column_name).cast("date"))

        df_spark = df_spark.drop(column_name)

    return df_spark


# üîπ Fun√ß√£o para salvar no BigQuery
def save_to_bigquery(df_pandas, bq_client, config, table_name):
    tabela_completa = f"{config['BQ_PROJECT']}.{config['BQ_DATASET']}.{table_name}"
    
    print(f"üöÄ Enviando dados para {tabela_completa}... (sem particionamento por nm_state)")
    
    try:
        to_gbq(
            df_pandas, 
            tabela_completa, 
            project_id=config['BQ_PROJECT'], 
            if_exists="replace",  # Mudar para 'append' se necess√°rio
            credentials=bq_client._credentials, 
            progress_bar=True
        )
        print(f"‚úÖ DataFrame salvo com sucesso no BigQuery: {tabela_completa}")
    except Exception as e:
        print(f"‚ùå Erro ao salvar no BigQuery: {e}")


# üîπ Fun√ß√£o principal para transforma√ß√£o Bronze ‚Üí Silver
def transform_to_silver(yaml_path):
    # Carregar configura√ß√£o e credenciais
    config, credentials = load_yaml(yaml_path)

    # Criar clientes para GCS e BigQuery
    storage_client = storage.Client(credentials=credentials)
    bq_client = bigquery.Client(credentials=credentials, project=config["BQ_PROJECT"])

    # Criar sess√£o Spark
    spark = SparkSession.builder.appName("Transform Bronze to Silver").getOrCreate()

    table_name = config["table_name"]

    # Encontra a √∫ltima parti√ß√£o no GCS
    latest_partition = get_latest_partition(storage_client, config["GCS_BUCKET"], table_name)

    # Carregar JSON do GCS para PySpark
    df_spark = load_json_from_gcs(storage_client, spark, config["GCS_BUCKET"], table_name, latest_partition)

    # Aplicar transforma√ß√µes
    df_silver_spark = apply_transforms(df_spark, config)

    # Converter para Pandas e salvar no BigQuery
    df_silver_pandas = df_silver_spark.toPandas()
    save_to_bigquery(df_silver_pandas, bq_client, config, table_name)

    print('‚úÖ Processo conclu√≠do.')



